{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='9f9ca55b-5722-4d4f-9d8b-d29585bd6302', project_access_token='p-2+UW4Gzdgvtt/Lp1SccoUozA==;MCJJwKnz21msfiiay2w+9Q==:onukmoy0CL1uNFha52uBBAuqLtuPij85tIzPiy9HWr5qIx4NyFte1LTjjpZaGmj29cx1YxoOia64+JJFMlOSaiQJld/Kn927Yw==')\npc = project.project_context\n\nfrom ibm_watson_studio_lib import access_project_or_space\nwslib = access_project_or_space({'token':'p-2+UW4Gzdgvtt/Lp1SccoUozA==;MCJJwKnz21msfiiay2w+9Q==:onukmoy0CL1uNFha52uBBAuqLtuPij85tIzPiy9HWr5qIx4NyFte1LTjjpZaGmj29cx1YxoOia64+JJFMlOSaiQJld/Kn927Yw=='})\n\n", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Ingest Files From Cloud Object Storage to Databases for Elasticsearch\n\nThis notebook handles file ingestion from a bucket in Cloud Object Storage instance to a Databases for Elasticsearch Platinum Plan instance using code derived from https://github.ibm.com/skol-assets/watsonx-wxd-setup-and-ingestion. Supported file types in the bucket include `pdf`, `docx`, `pptx`, `html`, and `txt`.\n\n**Prerequesites:**\n1. Sucessfully set up a connection to your Cloud Object Storage instance in the project\n2. Succesfully set up a connection to your Databases for Elasticsearch instance in the project\n3. Created an IBM Cloud API key in https://cloud.ibm.com/iam/apikeys and added it to your `Notebook_and_Deployment_Parameters` parameter set in the `ibm_cloud_apikey` field"}, {"metadata": {}, "cell_type": "markdown", "source": "**Before running any cells, connect the notebook to the rest of the project by inserting a project token.**\n\nYou can do this by clicking the three vertical dots next to the `Code Snippet` icon on the top right of the notebook UI and then clicking `Insert project token` to insert the token at the beginning of your notebook. Make sure to execute the new cell at the top of the notebook before running any other notebook cells."}, {"metadata": {}, "cell_type": "markdown", "source": "## Configure Notebook\n\nThe cells below configure the notebook by importing the necessary packages and adding the values configured in your project's Parameter Sets to the namespace. The following values within the `Notebook_and_Deployment_Parameters` parameter set will affect the output of running this notebook\n\n| Parameter Set Variable Name | Description |\n| --- | --- |\n| `ingestion_chunk_overlap` | The number of overlapping tokens between each document. |\n| `ingestion_chunk_size` | The maximum number of tokens each document will contain. |\n| `es_index_name` | The name of the Elasticsearch index where the ingested data will be stored. |\n| `es_index_text_field` | The name of the field that will store your document text in the Elasticsearch index. |\n| `es_model_name` | The name of the model in Elasticsearch that will be used for further processing or querying. |"}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport time\nimport warnings\nfrom typing import Optional, Tuple\n\nimport nest_asyncio\nimport nltk\nfrom elasticsearch import Elasticsearch, AsyncElasticsearch\nimport elasticsearch.exceptions\nimport elasticsearch.helpers\nimport elastic_transport\nfrom llama_index.core import VectorStoreIndex, StorageContext, Settings\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.readers.file import (\n    PDFReader,\n    DocxReader,\n    UnstructuredReader,\n    FlatReader,\n    HTMLTagReader,\n)\nfrom llama_index.vector_stores.elasticsearch import ElasticsearchStore\n\nwslib.download_file(\"utils.py\")\nimport utils\n\nnest_asyncio.apply()\nnltk.download(\"averaged_perceptron_tagger\")\nwarnings.filterwarnings(\"ignore\")", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/wsuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## PLEASE CHANGE THE INDEX NAME"}, {"metadata": {}, "cell_type": "code", "source": "# wslib_extension = utils.WSLibExtension(wslib)\n# params = wslib_extension.get_all_parameter_set_values()\nINGESTION_CHUNK_OVERLAP = 128 #params[\"ingestion_chunk_overlap\"]\nINGESTION_CHUNK_SIZE = 500 #params[\"ingestion_chunk_size\"]\nINDEX_NAME = \"rs-components-factsheet-rag\" #\"index-dy-competitors\" #params[\"es_index_name\"]\nINDEX_TEXT_FIELD = \"text_field\" #params[\"es_index_text_field\"]\nEMBEDDING_MODEL_NAME = \".elser_model_1\" #params[\"es_model_name\"]\nDEFAULT_READERS = {\n    \".pdf\": PDFReader(),\n    \".docx\": DocxReader(),\n    \".pptx\": UnstructuredReader(),\n    \".txt\": FlatReader(),\n    \".html\": HTMLTagReader(),\n}", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Read and Prepare Files From Cloud Object Storage\n\nThe cells below connect to your Cloud Object Storage bucket. Then, the files inside the bucket are read, chunked, and formatted into JSON objects which can be ingested by Watsonx Discovery. The cells use the LlamaIndex framework and [LlamaIndex file readers](https://llamahub.ai/l/readers/llama-index-readers-file?from=all) to perform the file ingestion. If you want to use another file reader for a particular extension, import the desired reader and edit the `DEFAULT_READERS` dictionary with the reader you want to use. The default schema for the ingested JSON objects is described below for reference. In addition to the schema below, this notebook also adds `url` and `file_name` to each document to ease the integration with the Watsonx Assistant native search extension.\n\n| Level 1 Key | Level 2 Key | Level 3 Key | Level 4 Key | Description |\n| --- | --- | --- | --- | --- |\n| `_id` | | | | The unique identifier of the document. |\n| `_source` | | | | Contains the main data of the document. |\n| `_source` | `hash` | | | The hash of the document. |\n| `_source` | `metadata` | | | Contains metadata about the document. |\n| `_source` | `metadata` | `file_name` | | The name of the file from which the document was created. |\n| `_source` | `metadata` | `page_label` | | The label of the page from which the document was created. |\n| `_source` | `relationships` | | | Contains relationships of the document with other documents. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | | Contains information about the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `hash` | The hash of the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `metadata` | Metadata of the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `node_id` | The unique identifier of the next document. |\n| `_source` | `relationships` | `NodeRelationship.NEXT` | `node_type` | The type of the next document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | | Contains information about the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `hash` | The hash of the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `metadata` | Metadata of the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `node_id` | The unique identifier of the source document. |\n| `_source` | `relationships` | `NodeRelationship.SOURCE` | `node_type` | The type of the source document. |\n| `_source` | `text_field` | | | The text content of the document. |"}, {"metadata": {}, "cell_type": "markdown", "source": "### Connect to Cloud Object Storage"}, {"metadata": {}, "cell_type": "code", "source": "# cos_connection_dict = wslib.get_connection(\"CloudObjectStorage\")\n# cos_auth_dict = json.loads(cos_connection_dict[\"credentials\"])\n\n# cos_reader = utils.CloudObjectStorageReader(\n#     bucket_name=cos_connection_dict[\"bucket\"],\n#     credentials={\n#         \"apikey\": cos_auth_dict[\"apikey\"],\n#         \"service_instance_id\": cos_auth_dict[\"resource_instance_id\"],\n#     },\n#     hostname=f\"https://{cos_connection_dict['url']}\",\n#     file_extractor=DEFAULT_READERS,\n# )", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Read files from Cloud Object Storage and convert them into LlamaIndex document objects"}, {"metadata": {}, "cell_type": "code", "source": "# documents = cos_reader.load_data(show_progress=True)\n# print(f\"Total documents: {len(documents)}\\nExample document:\\n{documents[0]}\")", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install ibm_watson --quiet", "execution_count": 22, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install llama-index --quiet", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from llama_index.core import Document\n\n# text_list = [text1, text2, ...]\n# documents = [Document(text=t) for t in text_list]", "execution_count": 24, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%time\n\nfrom ibm_watson import DiscoveryV2\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nimport os\nimport time\n\n\nos.environ[\"WD_API_KEY\"] = \"9h8dcbTEISwGtgyQtSIGs5jQJJS11ZcVsKhMxfB0IxA8\"\nos.environ[\"WD_SERVICE_URL\"] = \"https://api.us-south.discovery.watson.cloud.ibm.com/instances/4371dad1-d45e-4f61-b10c-b734779ba009\"\n\n\n# #this is for rs-components Factsheet\nos.environ[\"WD_PROJECT_ID\"] = \"15c84103-da6a-4e74-8681-0a2d375b1005\"\nos.environ[\"WD_COLLECTION_ID\"] = \"b48aadad-6628-4f40-0000-018ef1ba3bd3\"\n\n\n\n\n\nMAX_RETRIES = 3\nWD_PAGE_SIZE = 20\nPAST_N_DAYS = 365\n\nauthenticator = IAMAuthenticator(os.environ[\"WD_API_KEY\"])\nwd_client = DiscoveryV2(\n    version=\"2020-08-30\",\n    authenticator=authenticator\n)\n    \nwd_client.set_service_url(os.environ[\"WD_SERVICE_URL\"])\n\npage_id = 0\nretries = 0\ndocuments = []\nwhile 1:\n    try:\n        response = wd_client.query(\n            project_id=os.environ[\"WD_PROJECT_ID\"],\n            collection_ids=[os.environ[\"WD_COLLECTION_ID\"]],\n            return_=[\"text\", \"metadata\"],\n            #filter=\"metadata.source.LastModified>\" + str(date_from),\n            count=WD_PAGE_SIZE,\n            offset=page_id*WD_PAGE_SIZE\n        ).get_result()\n        if response is None or not isinstance(response, dict):\n            raise ValueError(\"No query result\")\n        if \"results\" not in response or response[\"results\"] is None or not isinstance(response[\"results\"], list):\n            raise ValueError(\"No query result\")\n        results = response[\"results\"]\n        if len(results) == 0:\n            break\n        _temp = []\n        for val in results:\n            if \"text\" in val:\n                _temp.append(Document(text=val[\"text\"][0]))\n#                 _temp.append({\"text\":val[\"text\"], \"parent_document_id\": val[\"metadata\"][\"parent_document_id\"]})\n#                 print(val[\"metadata\"])\n        documents.extend(_temp)\n        page_id += 1\n    except Exception as error:\n        print(error)\n        retries += 1\n        print(\"Re Trying.....\")\n        time.sleep(5)\n        if retries > MAX_RETRIES:\n            break\n        \n        \n        \n        \n        \n\nlen(documents)", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "CPU times: user 84.7 ms, sys: 4.17 ms, total: 88.9 ms\nWall time: 1.76 s\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 25, "data": {"text/plain": "1"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "len(documents)", "execution_count": 26, "outputs": [{"output_type": "execute_result", "execution_count": 26, "data": {"text/plain": "1"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "documents[0]", "execution_count": 27, "outputs": [{"output_type": "execute_result", "execution_count": 27, "data": {"text/plain": "Document(id_='5c1bf6fe-e93d-42c4-b808-9e73230f6298', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Factsheets- Client Name\\nBusiness Overview\\nName:\\n RS Components (formerly part of\\nElectrocomponents plc)\\nIndustry:\\n Distributor of industrial and electronic\\nproducts\\nFounded:\\n 1937 (as Radiospares Limited)\\nHeadquarters:\\n London, United Kingdom\\nWebsite:\\n https://www.rs-online.com/\\nAnnual Revenue:\\n Not publicly available information\\nfor RS Components specifically, but RS Group plc (their parent company)\\nrevenue can be found in their financial reports.\\nCompetitors:\\n Farnell element14, Digi-Key, Mouser\\nElectronics, Newark Electronics, and Avnet.\\nTotal Web Traffic: 555000\\nWappalyzer\\nCDN:\\nAnalytics:\\nAdvertising:\\nBuiltwith\\nAnalytics: Leadfeeder\\nCMS:\\nCDN:\\nCustomer Profile:\\nRS Components, a distributor of industrial and electronic products\\nwith a global presence, is navigating an environment of rising costs,\\nchip shortages, and e-commerce competition.\\nThey are strategically focusing on value-added services, product\\nportfolio expansion in areas like automation, and an enhanced e-commerce\\nplatform to drive growth.\\nThe increasing demand for electronics across various industries and\\nthe rise of automation present significant opportunities for RS.\\nMaintaining profitability, adapting to rapid technological change, and\\nattracting skilled talent are ongoing challenges for RS.\\nIndustry Headwins:\\nGlobal Supply Chain Disruptions: The ongoing chip shortage and\\nlogistical challenges are impacting the availability and pricing of\\nelectronic components, a core product line for RS.\\nRising Material Costs: Inflation and increasing raw material costs\\ncan squeeze margins and force RS to raise prices, potentially impacting\\ncustomer demand.\\nConsolidation in the Electronics Industry: Mergers and acquisitions\\namong manufacturers can limit RS\u2019s bargaining power and access to\\ncertain products.\\nCompetition from Online Retailers: E-commerce giants like Amazon are\\nincreasingly entering the industrial and electronic supplies market,\\nputting pressure on RS\u2019s prices and market share.\\nStrategy:\\nFocus on Value-Added Services: RS is moving beyond just being a\\ndistributor by offering value-added services like design assistance,\\ntechnical support, and logistics management to differentiate themselves\\nand build stronger customer relationships.\\nExpanding Product Portfolio: To cater to a wider range of customer\\nneeds, RS is strategically expanding their product offerings beyond\\ntraditional electronic components into areas like industrial automation,\\ntest and measurement equipment, and maintenance supplies.\\nEnhancing E-commerce Platform: To compete effectively online, RS is\\ncontinuously investing in improving their e-commerce platform, offering\\nuser-friendly interfaces, fast product search, and efficient ordering\\nprocesses.\\nGlobal Expansion: RS is looking to expand its reach into new\\nmarkets, particularly in emerging economies with growing demand for\\nindustrial and electronic products.\\nDrivers of Growth:\\nGrowth in Electronics Demand: The widespread adoption of electronics\\nin industries like healthcare, automotive, and renewable energy is a\\nmajor tailwind for RS. As these sectors expand, the need for the\\ncomponents they sell will increase.\\nRise of Automation: The increasing trend of automation across\\nmanufacturing and other sectors creates a strong demand for industrial\\nautomation products, which is a specific area of focus for RS\u2019s growth\\nstrategy.\\nE-commerce Boom: The overall shift towards online shopping is\\nexpected to benefit RS\u2019s e-commerce platform significantly, potentially\\nleading to substantial sales growth.\\nInvestment in Infrastructure: Government spending on infrastructure\\nprojects like smart cities and renewable energy could significantly\\nincrease demand for electrical and electronic components, a core product\\nline for RS.\\nBussiness Challenges:\\nGrowth in Electronics Demand: The widespread adoption of electronics\\nin industries like healthcare, automotive, and renewable energy is a\\nmajor tailwind for RS. As these sectors expand, the need for the\\ncomponents they sell will increase.\\nRise of Automation: The increasing trend of automation across\\nmanufacturing and other sectors creates a strong demand for industrial\\nautomation products, which is a specific area of focus for RS\u2019s growth\\nstrategy.\\nE-commerce Boom: The overall shift towards online shopping is\\nexpected to benefit RS\u2019s e-commerce platform significantly, potentially\\nleading to substantial sales growth.\\nInvestment in Infrastructure: Government spending on infrastructure\\nprojects like smart cities and renewable energy could significantly\\nincrease demand for electrical and electronic components, a core product\\nline for RS.\\nAnnual Report:\\nNot available, see reports of parent company RS Group\\nRecent News:\\nFinance\\ndirector of RS Group resigns over relationship with colleague\\nRS\\nunveils Better World product range to help customers select more\\nsustainable products\\nProactive\\nmaintenance and energy reduction strategies will help food and beverage\\nmanufacturers relieve mounting cost pressures\\nRS\\nadds cloud-based circuit simulation to DesignSpark tool suite\\nKey Stakeholders:\\nPete Malpas: President of EMEA\\n (Europe, Middle East,\\nand Africa) for RS Group, overseeing a vast region and likely\\nresponsible for driving sales and market share growth within that\\nterritory.\\nDoug Moody: President of the Americas for RS Group\\n,\\nmanaging the operations and performance of RS Components across North\\nand South America.\\nSean Fredericks: President of Asia Pacific for RS\\nGroup\\n, leading the expansion and strategic direction of RS in\\nthe high-growth economies of that region.\\nJason Taylor: Chief Information Officer (CIO) for RS\\nGroup\\n, responsible for overseeing the company\u2019s technology\\ninfrastructure, ensuring efficient operations and supporting the growth\\ninitiatives across all regions.\\nCompetitors Reports:\\nAB Tasty\\nIn comparison to AB Tasty, Dynamic Yield has several key advantages\\nin personalization capabilities, partnerships, and pricing. Here are\\nsome of the critical facts that highlight how Dynamic Yield can win over\\nAB Tasty:\\nAdvanced personalization capabilities: Dynamic Yield offers a more\\nsophisticated personalization solution than AB Tasty, with an end-to-end\\napproach that includes a wide range of touchpoints and data\\ncollection.\\nStrategic partnerships: Dynamic Yield\u2019s partnerships are seen as\\nmore strategic and long-term, with co-marketing opportunities and\\nflexible deployment modes.\\nIndustry-leading personalization solution: Dynamic Yield\u2019s\\npersonalization solution, backed by Mastercard\u2019s acquisition, provides\\nreach and accessibility to brands.\\nAppealing pricing model: Dynamic Yield\u2019s pricing model is appealing\\nand straightforward, with a focus on MUVs.\\nMastercard acquisition: Dynamic Yield\u2019s acquisition by Mastercard\\nprovides a strong backing and a wider reach to the solution.\\nMore sophisticated solution: Dynamic Yield\u2019s solution meets\\nmarketers\u2019 needs for experimentation, agility, and full control over\\ncontent and KPIs.\\nOptimizely\\nCompetitors: - Skims, a US Automotive Retailer, and a Spanish\\nSupermarket chose Dynamic Yield over Optimizely due to its more robust\\nrecommendations and reporting capabilities, as well as its long-term\\npartnership approach. - Dynamic Yield provides a more unified approach\\nto personalization, as opposed to Optimizely\u2019s modular approach. -\\nDynamic Yield has a roadmap for recommendations, optimization,\\nmessaging, and email, showing its commitment to scaling with the\\ncustomer\u2019s business. - Optimizely is known to pressure customers to\\nclose deals quickly and use steep discounts as incentives. Dynamic Yield\\ncan use this knowledge to its advantage by understanding the customer\u2019s\\ntimeline and using urgency appropriately, while also highlighting the\\nlong-term benefits of partnering with Dynamic Yield.\\nAbout Partners:\\namplience\\nSeamless content delivery and personalization for engaging\\ncustomer experiences.\\nFlexible deployment modes, including client-side, server-side,\\nand hybrid.\\nExtensive and growing product integrations with best-of-breed\\ntech partners, including Amplience.\\nCo-marketing opportunities with Mastercard through events, PR,\\nand content.\\nLeverage the full potential of technology stack with the\\nintegration.\\nMastercard DY can leverage Amplience\u2019s content management system\\n(CMS) to deliver personalized content to customers, improving their\\ndigital experience.\\nAmplience\u2019s integration with Mastercard DY can enable real-time\\ndata synchronization, allowing for more accurate and timely\\npersonalization.\\nMastercard DY can benefit from Amplience\u2019s scalability and\\nreliability, ensuring seamless content delivery even during peak\\ntraffic.\\nThe integration with Amplience can provide Mastercard DY with\\nadvanced analytics and reporting capabilities, enabling data-driven\\ndecision making.\\nMastercard DY can utilize Amplience\u2019s Dynamic Media capabilities\\nto deliver high-quality images and videos, enhancing the overall\\ncustomer experience.\\nGoogle Analytics\\nReal-time analysis of audiences through integration with Google\\nAnalytics, enabling more informed decision-making.\\nNative integration with Google Analytics, enhancing the\\ncredibility of results through analytics.\\nAffinity profiles and audience-based segmentation, leading to\\nmore effective personalization and testing.\\nAccess to Google Analytics\u2019 extensive data and analytics\\ncapabilities, providing a more comprehensive understanding of user\\nbehavior.\\nImproved measurement and tracking of user engagement and\\nconversion through integration with Google Analytics.\\nGoogle Analytics integration with Optimizely\u2019s Personalization\\nand Experimentation products allows for advanced reporting and\\nprediction insights, addressing Optimizely\u2019s limitations.\\nThe integration simplifies the implementation process and reduces\\nthe need for additional resources and tools.\\nGoogle Analytics provides a more user-friendly SDK model compared\\nto Optimizely, making it easier to implement and manage.\\nThe integration allows for a more streamlined process for\\ncreating and launching experiments, reducing the need for external tools\\nand additional costs.\\nGoogle Analytics\u2019 advanced reporting capabilities provide a more\\ncomprehensive view of customer behavior and preferences, allowing for\\nmore informed decision making and optimization strategies.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Ingest chunked and formatted files into Watsonx Discovery \n\nNow that we have prepared the documents to be ingested into Watsonx Discovery, the next step is to actually ingest the files. To do this, we first need to connect to our Databases for Elasticsearch service using the connection in the project. Then, we perform the following steps to prepare the service:\n\n1. Try to download and deploy the ELSER model in the Databases for Elasticsearch service\n2. Create an index to store the documents\n3. Create a pipeline to specify the use of the ELSER model to create vector embeddings on document ingestion\n\nOnce these steps are done, we finally ingest the documents above into the index using the pipeline"}, {"metadata": {}, "cell_type": "markdown", "source": "### Connect to Watsonx Discovery"}, {"metadata": {}, "cell_type": "code", "source": "wxd_connection_dict = wslib.get_connection(\"WatsonxDiscovery\")\n\nes_client = Elasticsearch(\n    wxd_connection_dict[\"url\"],\n    basic_auth=(wxd_connection_dict[\"username\"], wxd_connection_dict[\"password\"]),\n    verify_certs=False,\n    request_timeout=3600,\n)\nasync_es_client = AsyncElasticsearch(\n    wxd_connection_dict[\"url\"],\n    basic_auth=(wxd_connection_dict[\"username\"], wxd_connection_dict[\"password\"]),\n    verify_certs=False,\n    request_timeout=3600,\n)\nes_client.info()", "execution_count": 28, "outputs": [{"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "ObjectApiResponse({'name': 'm-2.8d5add5d-7f59-450e-80be-39d090c84cb5.34f333bb5fdb4697a48af756e824e490.bkvfu0nd0m8k95k94ujg.databases.appdomain.cloud', 'cluster_name': '8d5add5d-7f59-450e-80be-39d090c84cb5', 'cluster_uuid': 'CmWvF5msQHeUKiqNZHK2_w', 'version': {'number': '8.10.1', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': 'a94744f97522b2b7ee8b5dc13be7ee11082b8d6b', 'build_date': '2023-09-14T20:16:27.027355296Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Set up embedding model, index, and ingestion pipeline"}, {"metadata": {}, "cell_type": "code", "source": "def download_model(\n    client: Elasticsearch, model_id: str, model_text_field: str = \"text\"\n) -> None:\n    \"\"\"\n    Downloads a trained model from elasticsearch.Elasticsearch if it doesn't already exist.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        model_id (str): The ID of the trained model.\n        model_text_field (str): The name of the field the model looks for text to embed by default.\n\n    \"\"\"\n    try:\n        client.ml.get_trained_models(\n            model_id=model_id\n        )  # Throws error if model_id does not exist\n        print(f\"{model_id} already downloaded...\")\n    except elasticsearch.exceptions.NotFoundError:\n        model_schema = {\"input\": {\"field_names\": [model_text_field]}}\n        print(f\"Downloading {model_id}...\")\n        client.ml.put_trained_model(model_id=model_id, body=model_schema)\n        time.sleep(90)  # Wait for the model to be downloaded\n\n\ndef deploy_model(\n    client: Elasticsearch,\n    model_id: str,\n    deployment_id: Optional[str] = None,\n) -> None:\n    \"\"\"\n    Deploys a model through the Elasticsearch client.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        model_id (str): The ID of the model to deploy.\n        deployment_id (str, optional): The ID to use for the deployment. Defaults to the model_id.\n\n    \"\"\"\n    if not deployment_id:\n        deployment_id = model_id\n\n    existing_deployments = (\n        client.ml.get_trained_models_stats(model_id=model_id)\n        .body[\"trained_model_stats\"][0]\n        .get(\"deployment_stats\")\n    )\n    if (\n        existing_deployments\n        and existing_deployments.get(\"deployment_id\") == deployment_id\n    ):\n        print(f\"{model_id} model deployment with the same name already exists.\")\n    else:\n        print(\n            f\"Creating {model_id} model deployment with deployment id {deployment_id}...\"\n        )\n        client.ml.start_trained_model_deployment(\n            model_id=model_id, deployment_id=deployment_id\n        )\n\n\ndef get_model_text_field(client: Elasticsearch, model_id: str) -> str:\n    \"\"\"\n    Retrieves the text field name from a trained model configuration.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        model_id (str): The ID of the trained model.\n\n    Returns:\n        str: The name of the text field.\n    \"\"\"\n    response = client.ml.get_trained_models(model_id=model_id)\n    return response.body[\"trained_model_configs\"][0][\"input\"][\"field_names\"][0]\n\n\ndef create_index_with_default_pipeline(\n    client: Elasticsearch,\n    index_name: str,\n    index_config: dict,\n    pipeline_config: dict,\n    pipeline_name: str = \"default_pipeline\",\n) -> Tuple[elastic_transport.ObjectApiResponse, elastic_transport.ObjectApiResponse]:\n    \"\"\"\n    Creates an elasticsearch.Elasticsearch index with a default ingestion pipeline.\n\n    Args:\n        client (elasticsearch.Elasticsearch): The elasticsearch.Elasticsearch client.\n        index_name (str): The name of the index to be created.\n        index_config (dict): The configuration settings for the index.\n        pipeline_config (dict): The configuration settings for the ingestion pipeline.\n        pipeline_name (str, optional): The name of the ingestion pipeline. Defaults to \"default_pipeline\".\n\n    Returns:\n        tuple: A tuple containing the index creation response and the pipeline creation response.\n    \"\"\"\n    pipeline_name = pipeline_name or \"default_pipeline\"\n\n    if client.indices.exists(index=index_name):\n        print(f\"Deleting the existing index {index_name}...\")\n        client.indices.delete(index=index_name)\n\n    print(\"Creating the ingestion pipeline...\")\n    pipeline_response = client.ingest.put_pipeline(\n        id=pipeline_name, body=pipeline_config\n    )\n\n    print(\"Creating the index...\")\n    index_config[\"settings\"] = {\n        \"index.default_pipeline\": pipeline_name,\n    }\n    index_response = client.indices.create(index=index_name, body=index_config)\n    return index_response, pipeline_response", "execution_count": 29, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "download_model(es_client, EMBEDDING_MODEL_NAME, INDEX_TEXT_FIELD)\ndeploy_model(es_client, EMBEDDING_MODEL_NAME)\nembedding_model_text_field = get_model_text_field(es_client, EMBEDDING_MODEL_NAME)\n\nindex_config = {\n    \"mappings\": {\n        \"properties\": {\n            \"ml.tokens\": {\"type\": \"rank_features\"},\n            INDEX_TEXT_FIELD: {\"type\": \"text\"},\n        }\n    }\n}\n\npipeline_config = {\n    \"description\": \"Inference pipeline using ELSER embedding model\",\n    \"processors\": [\n        # Calculate embeddings\n        {\n            \"inference\": {\n                \"field_map\": {INDEX_TEXT_FIELD: embedding_model_text_field},\n                \"model_id\": EMBEDDING_MODEL_NAME,\n                \"target_field\": \"ml\",\n                \"inference_config\": {\"text_expansion\": {\"results_field\": \"tokens\"}},\n            }\n        },\n        # Give file_name and url their own fields for native Watsonx Assistant integration\n        {\"set\": {\"field\": \"file_name\", \"value\": \"{{metadata.file_name}}\"}},\n        {\"set\": {\"field\": \"url\", \"value\": \"{{metadata.url}}\"}},\n    ],\n    \"version\": 1,\n}", "execution_count": 30, "outputs": [{"output_type": "stream", "text": ".elser_model_1 already downloaded...\n.elser_model_1 model deployment with the same name already exists.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "create_index_with_default_pipeline(es_client, INDEX_NAME, index_config, pipeline_config)", "execution_count": 31, "outputs": [{"output_type": "stream", "text": "Creating the ingestion pipeline...\nCreating the index...\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 31, "data": {"text/plain": "(ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'rs-components-factsheet-rag'}),\n ObjectApiResponse({'acknowledged': True}))"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Ingest the formatted documents"}, {"metadata": {}, "cell_type": "code", "source": "%%time\nSettings.embed_model = None\nSettings.llm = None\nSettings.node_parser = SentenceSplitter.from_defaults(\n    chunk_size=INGESTION_CHUNK_SIZE, chunk_overlap=INGESTION_CHUNK_OVERLAP\n)\n\nvector_store = ElasticsearchStore(\n    es_client=async_es_client, index_name=INDEX_NAME, text_field=INDEX_TEXT_FIELD\n)\ntry:\n    index = VectorStoreIndex.from_documents(\n        documents,\n        storage_context=StorageContext.from_defaults(vector_store=vector_store),\n        show_progress=True,\n    )\nexcept elasticsearch.helpers.BulkIndexError as e:\n    first_error = e.errors[0].get(\"index\", {}).get(\"error\", {})\n    print(f\"Bulk index error: {e}\")\n    print(\n        f\"Document ingestion to Elasticsearch failed with the following error: {first_error}\"\n    )", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "Embeddings have been explicitly disabled. Using MockEmbedding.\nLLM is explicitly disabled. Using MockLLM.\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "331b91e4871f48fc92b640b3939f8f04"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b3ffaf6b078d4b8ba0c579477dc9d075"}}, "metadata": {}}, {"output_type": "stream", "text": "CPU times: user 54.8 ms, sys: 873 \u00b5s, total: 55.7 ms\nWall time: 10.2 s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Test the ingested documents with a simple query"}, {"metadata": {}, "cell_type": "code", "source": "query_engine = index.as_query_engine(\n    similarity_top_k=3,\n    vector_store_query_mode=\"sparse\",\n    vector_store_kwargs={\n        \"custom_query\": utils.create_sparse_vector_query_with_model(\n            EMBEDDING_MODEL_NAME\n        )\n    },\n)\nresponse = query_engine.query(\"Tell me about Asos?\")\nfor source_node in response.source_nodes:\n#     print(f\"{source_node.metadata['file_name']}:\\n {source_node.text}\\n\\n\")\n    print(f\" {source_node.text}\\n\\n\")", "execution_count": 33, "outputs": [{"output_type": "stream", "text": " Bussiness Challenges:\nGrowth in Electronics Demand: The widespread adoption of electronics\nin industries like healthcare, automotive, and renewable energy is a\nmajor tailwind for RS. As these sectors expand, the need for the\ncomponents they sell will increase.\nRise of Automation: The increasing trend of automation across\nmanufacturing and other sectors creates a strong demand for industrial\nautomation products, which is a specific area of focus for RS\u2019s growth\nstrategy.\nE-commerce Boom: The overall shift towards online shopping is\nexpected to benefit RS\u2019s e-commerce platform significantly, potentially\nleading to substantial sales growth.\nInvestment in Infrastructure: Government spending on infrastructure\nprojects like smart cities and renewable energy could significantly\nincrease demand for electrical and electronic components, a core product\nline for RS.\nAnnual Report:\nNot available, see reports of parent company RS Group\nRecent News:\nFinance\ndirector of RS Group resigns over relationship with colleague\nRS\nunveils Better World product range to help customers select more\nsustainable products\nProactive\nmaintenance and energy reduction strategies will help food and beverage\nmanufacturers relieve mounting cost pressures\nRS\nadds cloud-based circuit simulation to DesignSpark tool suite\nKey Stakeholders:\nPete Malpas: President of EMEA\n (Europe, Middle East,\nand Africa) for RS Group, overseeing a vast region and likely\nresponsible for driving sales and market share growth within that\nterritory.\nDoug Moody: President of the Americas for RS Group\n,\nmanaging the operations and performance of RS Components across North\nand South America.\nSean Fredericks: President of Asia Pacific for RS\nGroup\n, leading the expansion and strategic direction of RS in\nthe high-growth economies of that region.\nJason Taylor: Chief Information Officer (CIO) for RS\nGroup\n, responsible for overseeing the company\u2019s technology\ninfrastructure, ensuring efficient operations and supporting the growth\ninitiatives across all regions.\nCompetitors Reports:\nAB Tasty\nIn comparison to AB Tasty, Dynamic Yield has several key advantages\nin personalization capabilities, partnerships, and pricing. Here are\nsome of the critical facts that highlight how Dynamic Yield can win over\nAB Tasty:\nAdvanced personalization capabilities: Dynamic Yield offers a more\nsophisticated personalization solution than AB Tasty, with an end-to-end\napproach that includes a wide range of touchpoints and data\ncollection.\n\n\n Factsheets- Client Name\nBusiness Overview\nName:\n RS Components (formerly part of\nElectrocomponents plc)\nIndustry:\n Distributor of industrial and electronic\nproducts\nFounded:\n 1937 (as Radiospares Limited)\nHeadquarters:\n London, United Kingdom\nWebsite:\n https://www.rs-online.com/\nAnnual Revenue:\n Not publicly available information\nfor RS Components specifically, but RS Group plc (their parent company)\nrevenue can be found in their financial reports.\nCompetitors:\n Farnell element14, Digi-Key, Mouser\nElectronics, Newark Electronics, and Avnet.\nTotal Web Traffic: 555000\nWappalyzer\nCDN:\nAnalytics:\nAdvertising:\nBuiltwith\nAnalytics: Leadfeeder\nCMS:\nCDN:\nCustomer Profile:\nRS Components, a distributor of industrial and electronic products\nwith a global presence, is navigating an environment of rising costs,\nchip shortages, and e-commerce competition.\nThey are strategically focusing on value-added services, product\nportfolio expansion in areas like automation, and an enhanced e-commerce\nplatform to drive growth.\nThe increasing demand for electronics across various industries and\nthe rise of automation present significant opportunities for RS.\nMaintaining profitability, adapting to rapid technological change, and\nattracting skilled talent are ongoing challenges for RS.\nIndustry Headwins:\nGlobal Supply Chain Disruptions: The ongoing chip shortage and\nlogistical challenges are impacting the availability and pricing of\nelectronic components, a core product line for RS.\nRising Material Costs: Inflation and increasing raw material costs\ncan squeeze margins and force RS to raise prices, potentially impacting\ncustomer demand.\nConsolidation in the Electronics Industry: Mergers and acquisitions\namong manufacturers can limit RS\u2019s bargaining power and access to\ncertain products.\nCompetition from Online Retailers: E-commerce giants like Amazon are\nincreasingly entering the industrial and electronic supplies market,\nputting pressure on RS\u2019s prices and market share.\nStrategy:\nFocus on Value-Added Services: RS is moving beyond just being a\ndistributor by offering value-added services like design assistance,\ntechnical support, and logistics management to differentiate themselves\nand build stronger customer relationships.\nExpanding Product Portfolio: To cater to a wider range of customer\nneeds, RS is strategically expanding their product offerings beyond\ntraditional electronic components into areas like industrial automation,\ntest and measurement equipment, and maintenance supplies.\n\n\n Competition from Online Retailers: E-commerce giants like Amazon are\nincreasingly entering the industrial and electronic supplies market,\nputting pressure on RS\u2019s prices and market share.\nStrategy:\nFocus on Value-Added Services: RS is moving beyond just being a\ndistributor by offering value-added services like design assistance,\ntechnical support, and logistics management to differentiate themselves\nand build stronger customer relationships.\nExpanding Product Portfolio: To cater to a wider range of customer\nneeds, RS is strategically expanding their product offerings beyond\ntraditional electronic components into areas like industrial automation,\ntest and measurement equipment, and maintenance supplies.\nEnhancing E-commerce Platform: To compete effectively online, RS is\ncontinuously investing in improving their e-commerce platform, offering\nuser-friendly interfaces, fast product search, and efficient ordering\nprocesses.\nGlobal Expansion: RS is looking to expand its reach into new\nmarkets, particularly in emerging economies with growing demand for\nindustrial and electronic products.\nDrivers of Growth:\nGrowth in Electronics Demand: The widespread adoption of electronics\nin industries like healthcare, automotive, and renewable energy is a\nmajor tailwind for RS. As these sectors expand, the need for the\ncomponents they sell will increase.\nRise of Automation: The increasing trend of automation across\nmanufacturing and other sectors creates a strong demand for industrial\nautomation products, which is a specific area of focus for RS\u2019s growth\nstrategy.\nE-commerce Boom: The overall shift towards online shopping is\nexpected to benefit RS\u2019s e-commerce platform significantly, potentially\nleading to substantial sales growth.\nInvestment in Infrastructure: Government spending on infrastructure\nprojects like smart cities and renewable energy could significantly\nincrease demand for electrical and electronic components, a core product\nline for RS.\nBussiness Challenges:\nGrowth in Electronics Demand: The widespread adoption of electronics\nin industries like healthcare, automotive, and renewable energy is a\nmajor tailwind for RS. As these sectors expand, the need for the\ncomponents they sell will increase.\nRise of Automation: The increasing trend of automation across\nmanufacturing and other sectors creates a strong demand for industrial\nautomation products, which is a specific area of focus for RS\u2019s growth\nstrategy.\nE-commerce Boom: The overall shift towards online shopping is\nexpected to benefit RS\u2019s e-commerce platform significantly, potentially\nleading to substantial sales growth.\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}